{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moderate-balance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Cardnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Merchnum</th>\n",
       "      <th>Merch description</th>\n",
       "      <th>Merch state</th>\n",
       "      <th>Merch zip</th>\n",
       "      <th>Transtype</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>...</th>\n",
       "      <th>card_desc_count_0_by_30</th>\n",
       "      <th>card_desc_count_1_by_7</th>\n",
       "      <th>card_desc_count_1_by_14</th>\n",
       "      <th>card_desc_count_1_by_30</th>\n",
       "      <th>merch_desc_count_0_by_7</th>\n",
       "      <th>merch_desc_count_0_by_14</th>\n",
       "      <th>merch_desc_count_0_by_30</th>\n",
       "      <th>merch_desc_count_1_by_7</th>\n",
       "      <th>merch_desc_count_1_by_14</th>\n",
       "      <th>merch_desc_count_1_by_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/23/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5142183973</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>61003026333</td>\n",
       "      <td>SERVICE MERCHANDISE #81</td>\n",
       "      <td>MA</td>\n",
       "      <td>1803</td>\n",
       "      <td>P</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5142131721</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>4503082993600</td>\n",
       "      <td>OFFICE DEPOT #191</td>\n",
       "      <td>MD</td>\n",
       "      <td>20706</td>\n",
       "      <td>P</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5142148452</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/28/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>5509006296254</td>\n",
       "      <td>FEDEX SHP 12/23/09 AB#</td>\n",
       "      <td>TN</td>\n",
       "      <td>38118</td>\n",
       "      <td>P</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96392</th>\n",
       "      <td>96749</td>\n",
       "      <td>5142276053</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>3500000006160</td>\n",
       "      <td>BEST BUY      00001610</td>\n",
       "      <td>KY</td>\n",
       "      <td>41042</td>\n",
       "      <td>P</td>\n",
       "      <td>84.79</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>1.136581</td>\n",
       "      <td>1.337872</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>1.061472</td>\n",
       "      <td>1.262391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96393</th>\n",
       "      <td>96750</td>\n",
       "      <td>5142225701</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>8090710030950</td>\n",
       "      <td>MARKUS OFFICE SUPPLIES</td>\n",
       "      <td>OH</td>\n",
       "      <td>45248</td>\n",
       "      <td>P</td>\n",
       "      <td>118.75</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071577</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>-1.693394</td>\n",
       "      <td>-1.177600</td>\n",
       "      <td>0.963189</td>\n",
       "      <td>-0.915423</td>\n",
       "      <td>-0.828069</td>\n",
       "      <td>0.871136</td>\n",
       "      <td>-1.102227</td>\n",
       "      <td>-0.939715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96394</th>\n",
       "      <td>96751</td>\n",
       "      <td>5142226486</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>4503057341100</td>\n",
       "      <td>TECH PAC, INC</td>\n",
       "      <td>OH</td>\n",
       "      <td>45150</td>\n",
       "      <td>P</td>\n",
       "      <td>363.56</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640298</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>0.606517</td>\n",
       "      <td>0.061288</td>\n",
       "      <td>0.037293</td>\n",
       "      <td>-0.683673</td>\n",
       "      <td>-0.138143</td>\n",
       "      <td>-0.097652</td>\n",
       "      <td>-0.792908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96395</th>\n",
       "      <td>96752</td>\n",
       "      <td>5142244619</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>8834000695412</td>\n",
       "      <td>BUY.COM</td>\n",
       "      <td>CA</td>\n",
       "      <td>92656</td>\n",
       "      <td>P</td>\n",
       "      <td>2202.03</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.642201</td>\n",
       "      <td>0.367905</td>\n",
       "      <td>0.476214</td>\n",
       "      <td>-1.772306</td>\n",
       "      <td>-1.291563</td>\n",
       "      <td>-1.231116</td>\n",
       "      <td>-1.134125</td>\n",
       "      <td>-0.138143</td>\n",
       "      <td>-0.810960</td>\n",
       "      <td>-1.083331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96396</th>\n",
       "      <td>96753</td>\n",
       "      <td>5142243247</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>9108347680006</td>\n",
       "      <td>STAPLES NATIONAL #471</td>\n",
       "      <td>NJ</td>\n",
       "      <td>7606</td>\n",
       "      <td>P</td>\n",
       "      <td>554.64</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.927514</td>\n",
       "      <td>-3.745815</td>\n",
       "      <td>-2.778199</td>\n",
       "      <td>-2.069659</td>\n",
       "      <td>-0.840613</td>\n",
       "      <td>-1.000924</td>\n",
       "      <td>-0.994679</td>\n",
       "      <td>-1.147422</td>\n",
       "      <td>-1.192381</td>\n",
       "      <td>-1.109108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96397 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recnum     Cardnum        Date       Merchnum        Merch description  \\\n",
       "0           1  5142190439  2010-01-01  5509006296254   FEDEX SHP 12/23/09 AB#   \n",
       "1           2  5142183973  2010-01-01    61003026333  SERVICE MERCHANDISE #81   \n",
       "2           3  5142131721  2010-01-01  4503082993600        OFFICE DEPOT #191   \n",
       "3           4  5142148452  2010-01-01  5509006296254   FEDEX SHP 12/28/09 AB#   \n",
       "4           5  5142190439  2010-01-01  5509006296254   FEDEX SHP 12/23/09 AB#   \n",
       "...       ...         ...         ...            ...                      ...   \n",
       "96392   96749  5142276053  2010-12-31  3500000006160   BEST BUY      00001610   \n",
       "96393   96750  5142225701  2010-12-31  8090710030950   MARKUS OFFICE SUPPLIES   \n",
       "96394   96751  5142226486  2010-12-31  4503057341100            TECH PAC, INC   \n",
       "96395   96752  5142244619  2010-12-31  8834000695412                  BUY.COM   \n",
       "96396   96753  5142243247  2010-12-31  9108347680006    STAPLES NATIONAL #471   \n",
       "\n",
       "      Merch state Merch zip Transtype   Amount  Fraud  ...  \\\n",
       "0              TN     38118         P     3.62      0  ...   \n",
       "1              MA      1803         P    31.42      0  ...   \n",
       "2              MD     20706         P   178.49      0  ...   \n",
       "3              TN     38118         P     3.62      0  ...   \n",
       "4              TN     38118         P     3.62      0  ...   \n",
       "...           ...       ...       ...      ...    ...  ...   \n",
       "96392          KY     41042         P    84.79      0  ...   \n",
       "96393          OH     45248         P   118.75      0  ...   \n",
       "96394          OH     45150         P   363.56      0  ...   \n",
       "96395          CA     92656         P  2202.03      0  ...   \n",
       "96396          NJ      7606         P   554.64      0  ...   \n",
       "\n",
       "       card_desc_count_0_by_30 card_desc_count_1_by_7  \\\n",
       "0                     0.640298               0.367905   \n",
       "1                     0.640298               0.367905   \n",
       "2                     0.640298               0.367905   \n",
       "3                     0.640298               0.367905   \n",
       "4                     0.640298               0.367905   \n",
       "...                        ...                    ...   \n",
       "96392                 0.640298               0.367905   \n",
       "96393                -1.071577               0.367905   \n",
       "96394                 0.640298               0.367905   \n",
       "96395                -1.642201               0.367905   \n",
       "96396                -1.927514              -3.745815   \n",
       "\n",
       "       card_desc_count_1_by_14  card_desc_count_1_by_30  \\\n",
       "0                     0.476214                 0.606517   \n",
       "1                     0.476214                 0.606517   \n",
       "2                     0.476214                 0.606517   \n",
       "3                     0.476214                 0.606517   \n",
       "4                     0.476214                 0.606517   \n",
       "...                        ...                      ...   \n",
       "96392                 0.476214                 0.606517   \n",
       "96393                -1.693394                -1.177600   \n",
       "96394                 0.476214                 0.606517   \n",
       "96395                 0.476214                -1.772306   \n",
       "96396                -2.778199                -2.069659   \n",
       "\n",
       "       merch_desc_count_0_by_7  merch_desc_count_0_by_14  \\\n",
       "0                     0.963189                  1.136581   \n",
       "1                     0.963189                  1.136581   \n",
       "2                     0.963189                  1.136581   \n",
       "3                     0.963189                  1.136581   \n",
       "4                     0.963189                  1.136581   \n",
       "...                        ...                       ...   \n",
       "96392                 0.963189                  1.136581   \n",
       "96393                 0.963189                 -0.915423   \n",
       "96394                 0.061288                  0.037293   \n",
       "96395                -1.291563                 -1.231116   \n",
       "96396                -0.840613                 -1.000924   \n",
       "\n",
       "       merch_desc_count_0_by_30  merch_desc_count_1_by_7  \\\n",
       "0                      1.337872                 0.871136   \n",
       "1                      1.337872                 0.871136   \n",
       "2                      1.337872                 0.871136   \n",
       "3                      1.337872                 0.871136   \n",
       "4                      1.337872                 0.871136   \n",
       "...                         ...                      ...   \n",
       "96392                  1.337872                 0.871136   \n",
       "96393                 -0.828069                 0.871136   \n",
       "96394                 -0.683673                -0.138143   \n",
       "96395                 -1.134125                -0.138143   \n",
       "96396                 -0.994679                -1.147422   \n",
       "\n",
       "       merch_desc_count_1_by_14  merch_desc_count_1_by_30  \n",
       "0                      1.061472                  1.262391  \n",
       "1                      1.061472                  1.262391  \n",
       "2                      1.061472                  1.262391  \n",
       "3                      1.061472                  1.262391  \n",
       "4                      1.061472                  1.262391  \n",
       "...                         ...                       ...  \n",
       "96392                  1.061472                  1.262391  \n",
       "96393                 -1.102227                 -0.939715  \n",
       "96394                 -0.097652                 -0.792908  \n",
       "96395                 -0.810960                 -1.083331  \n",
       "96396                 -1.192381                 -1.109108  \n",
       "\n",
       "[96397 rows x 561 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df3.csv').drop(columns = 'Unnamed: 0')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sexual-flexibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>card_desc_total_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>card_state_max_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>card_state_total_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>card_zip_max_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>merch_desc_total_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>card_desc_total_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>card_desc_total_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cardnum_total_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>merch_state_total_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>card_merch_max_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>merch_state_total_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>card_zip_total_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>card_zip_max_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>merch_desc_max_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>merch_state_max_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>merch_desc_max_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>card_state_total_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>merch_zip_max_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Merchnum_total_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>merch_zip_total_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>card_state_max_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>card_zip_total_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>merch_desc_total_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>merch_state_total_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Merchnum_total_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>card_merch_total_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>merch_zip_max_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>card_zip_max_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cardnum_total_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>merch_desc_max_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Variable\n",
       "0     card_desc_total_3\n",
       "1     card_state_max_30\n",
       "2    card_state_total_7\n",
       "3        card_zip_max_1\n",
       "4    merch_desc_total_0\n",
       "5    card_desc_total_30\n",
       "6    card_desc_total_14\n",
       "7       Cardnum_total_0\n",
       "8   merch_state_total_1\n",
       "9      card_merch_max_1\n",
       "10  merch_state_total_3\n",
       "11    card_zip_total_14\n",
       "12      card_zip_max_14\n",
       "13     merch_desc_max_0\n",
       "14    merch_state_max_1\n",
       "15     merch_desc_max_3\n",
       "16  card_state_total_30\n",
       "17      merch_zip_max_3\n",
       "18     Merchnum_total_1\n",
       "19    merch_zip_total_1\n",
       "20     card_state_max_7\n",
       "21     card_zip_total_1\n",
       "22   merch_desc_total_1\n",
       "23  merch_state_total_0\n",
       "24     Merchnum_total_3\n",
       "25  card_merch_total_30\n",
       "26      merch_zip_max_0\n",
       "27      card_zip_max_30\n",
       "28      Cardnum_total_7\n",
       "29     merch_desc_max_1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = pd.read_csv('vars_30.csv').drop(columns = 'Unnamed: 0')\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absolute-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.loc[30, 'Variable'] = 'Fraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date=pd.to_datetime(df.Date)\n",
    "\n",
    "model_data = df[(df['Date'] < pd.to_datetime('2010-09-01'))]\n",
    "oot_data = df[(df['Date'] > pd.to_datetime('2010-09-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "neutral-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_selected = model_data.loc[: ,list(columns['Variable'])]\n",
    "oot_data_selectded = oot_data[list(columns['Variable'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wicked-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = model_data['Fraud'].value_counts().plot(kind='bar', figsize=(10, 6), fontsize=13, color='#087E8B')\n",
    "ax.set_title('Credit card fraud (0 = normal, 1 = fraud)', size=20, pad=30)\n",
    "ax.set_ylabel('Number of transactions', fontsize=14)\n",
    "\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x() + 0.19, i.get_height() + 700, str(round(i.get_height(), 2)), fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "helpful-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= model_data_selected.drop('Fraud', axis=1)\n",
    "y= model_data_selected['Fraud'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "entitled-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR(model,i):\n",
    "    prop=0.03\n",
    "    len_train=round(len(x_train)*prop)\n",
    "    len_test=round(len(x_test)*prop)\n",
    "    len_oot=round(len(oot_data_selected)*prop)\n",
    "    \n",
    "    pred_train=model.predict_proba(x_train)[:,1]\n",
    "    lg_train=pd.DataFrame({'Pred': pred_train, 'Fraud':y_train}).sort_values(by='Pred',ascending=False).head(len_train)\n",
    "    \n",
    "    pred_test=model.predict_proba(x_test)[:,1]\n",
    "    lg_test=pd.DataFrame({'Pred': pred_test, 'Fraud':y_test}).sort_values(by='Pred',ascending=False).head(len_test)\n",
    "    \n",
    "    pred_oot=model.predict_proba(oot_data_selected.drop('Fraud',axis=1))[:,1]\n",
    "    lg_oot=pd.DataFrame({'Pred': pred_oot, 'Fraud':oot_data_selected.Fraud}).sort_values(by='Pred',ascending=False).head(len_oot)\n",
    "    \n",
    "    FDR_train=sum(lg_train.Fraud)/sum(y_train)\n",
    "    FDR_test=sum(lg_test.Fraud)/sum(y_test)\n",
    "    FDR_oot=sum(lg_oot.Fraud)/sum(oot_data_selected.Fraud)\n",
    "    \n",
    "    df_FDR=pd.DataFrame({'Train': FDR_train, 'Test':FDR_test,'OOT':FDR_oot},index=[i])\n",
    "    \n",
    "    return (df_FDR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-enough",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "empirical-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_logistic=pd.DataFrame(np.zeros(shape=(10,6)),columns=[\"C\",\"penalty\",\"solver\",\"Train\",\"Test\",\"OOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "silver-stability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.718884</td>\n",
       "      <td>0.641350</td>\n",
       "      <td>0.584270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.708696</td>\n",
       "      <td>0.658436</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.695556</td>\n",
       "      <td>0.667984</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.632558</td>\n",
       "      <td>0.525281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.690987</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>0.696833</td>\n",
       "      <td>0.573034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.692149</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.695455</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.706366</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.519663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.694093</td>\n",
       "      <td>0.703057</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C penalty     solver     Train      Test       OOT\n",
       "0  1      l1  liblinear  0.718884  0.641350  0.584270\n",
       "1  1      l1  liblinear  0.708696  0.658436  0.544944\n",
       "2  1      l1  liblinear  0.695556  0.667984  0.542135\n",
       "3  1      l1  liblinear  0.709016  0.632558  0.525281\n",
       "4  1      l1  liblinear  0.690987  0.708861  0.544944\n",
       "5  1      l1  liblinear  0.692946  0.696833  0.573034\n",
       "6  1      l1  liblinear  0.692149  0.666667  0.525281\n",
       "7  1      l1  liblinear  0.681159  0.695455  0.544944\n",
       "8  1      l1  liblinear  0.706366  0.652778  0.519663\n",
       "9  1      l1  liblinear  0.694093  0.703057  0.542135"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"1\"\n",
    "df_logistic['penalty']=\"l1\"\n",
    "df_logistic['solver']=\"liblinear\"\n",
    "\n",
    "model = LogisticRegression(C=1,\n",
    "                           penalty = 'l1',\n",
    "                           solver=\"liblinear\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']= FDR(model,i)\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "advance-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.698985\n",
       "Test     0.672398\n",
       "OOT      0.544663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "compatible-neutral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.711253</td>\n",
       "      <td>0.650862</td>\n",
       "      <td>0.488764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.678647</td>\n",
       "      <td>0.734783</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.698031</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.578652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.525281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.709251</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.715640</td>\n",
       "      <td>0.676157</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.699580</td>\n",
       "      <td>0.709251</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.685106</td>\n",
       "      <td>0.690987</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C penalty     solver     Train      Test       OOT\n",
       "0  10      l1  liblinear  0.711253  0.650862  0.488764\n",
       "1  10      l1  liblinear  0.678647  0.734783  0.561798\n",
       "2  10      l1  liblinear  0.726115  0.612069  0.542135\n",
       "3  10      l1  liblinear  0.698031  0.699187  0.578652\n",
       "4  10      l1  liblinear  0.691796  0.698413  0.525281\n",
       "5  10      l1  liblinear  0.689076  0.709251  0.550562\n",
       "6  10      l1  liblinear  0.715640  0.676157  0.528090\n",
       "7  10      l1  liblinear  0.699580  0.709251  0.528090\n",
       "8  10      l1  liblinear  0.692308  0.729730  0.561798\n",
       "9  10      l1  liblinear  0.685106  0.690987  0.547753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"10\"\n",
    "df_logistic['penalty']=\"l1\"\n",
    "df_logistic['solver']=\"liblinear\"\n",
    "\n",
    "model = LogisticRegression(C=10,\n",
    "                           penalty = 'l1',\n",
    "                           solver=\"liblinear\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "removed-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.698755\n",
       "Test     0.691069\n",
       "OOT      0.541292\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "scientific-cream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.705051</td>\n",
       "      <td>0.663462</td>\n",
       "      <td>0.556180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.706140</td>\n",
       "      <td>0.668016</td>\n",
       "      <td>0.533708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.525281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.706263</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.584270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.656652</td>\n",
       "      <td>0.564607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.708889</td>\n",
       "      <td>0.667984</td>\n",
       "      <td>0.522472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.719912</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.530899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.744493</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.698690</td>\n",
       "      <td>0.661224</td>\n",
       "      <td>0.578652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.709130</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.587079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C penalty     solver     Train      Test       OOT\n",
       "0  100      l1  liblinear  0.705051  0.663462  0.556180\n",
       "1  100      l1  liblinear  0.706140  0.668016  0.533708\n",
       "2  100      l1  liblinear  0.683761  0.702128  0.525281\n",
       "3  100      l1  liblinear  0.706263  0.662500  0.584270\n",
       "4  100      l1  liblinear  0.712766  0.656652  0.564607\n",
       "5  100      l1  liblinear  0.708889  0.667984  0.522472\n",
       "6  100      l1  liblinear  0.719912  0.650407  0.530899\n",
       "7  100      l1  liblinear  0.672269  0.744493  0.550562\n",
       "8  100      l1  liblinear  0.698690  0.661224  0.578652\n",
       "9  100      l1  liblinear  0.709130  0.655172  0.587079"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"100\"\n",
    "df_logistic['penalty']=\"l1\"\n",
    "df_logistic['solver']=\"liblinear\"\n",
    "\n",
    "model = LogisticRegression(C=100,\n",
    "                           penalty = 'l1',\n",
    "                           solver=\"liblinear\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acknowledged-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.702287\n",
       "Test     0.673204\n",
       "OOT      0.553371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "comic-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.556180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.665957</td>\n",
       "      <td>0.656652</td>\n",
       "      <td>0.533708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.668103</td>\n",
       "      <td>0.707113</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.646288</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.672769</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.688841</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.684322</td>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.668094</td>\n",
       "      <td>0.690678</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.681070</td>\n",
       "      <td>0.695853</td>\n",
       "      <td>0.553371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.671772</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.530899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C penalty     solver     Train      Test       OOT\n",
       "0  0.01      l1  liblinear  0.692140  0.653061  0.556180\n",
       "1  0.01      l1  liblinear  0.665957  0.656652  0.533708\n",
       "2  0.01      l1  liblinear  0.668103  0.707113  0.550562\n",
       "3  0.01      l1  liblinear  0.683544  0.646288  0.542135\n",
       "4  0.01      l1  liblinear  0.672769  0.710526  0.539326\n",
       "5  0.01      l1  liblinear  0.688841  0.645570  0.539326\n",
       "6  0.01      l1  liblinear  0.684322  0.662338  0.561798\n",
       "7  0.01      l1  liblinear  0.668094  0.690678  0.539326\n",
       "8  0.01      l1  liblinear  0.681070  0.695853  0.553371\n",
       "9  0.01      l1  liblinear  0.671772  0.686992  0.530899"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"0.01\"\n",
    "df_logistic['penalty']=\"l1\"\n",
    "df_logistic['solver']=\"liblinear\"\n",
    "\n",
    "model = LogisticRegression(C=0.1,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"lbfgs\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "rough-stadium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.677661\n",
       "Test     0.675507\n",
       "OOT      0.544663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acquired-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.688421</td>\n",
       "      <td>0.688596</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.662602</td>\n",
       "      <td>0.522472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.682609</td>\n",
       "      <td>0.699588</td>\n",
       "      <td>0.553371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.713389</td>\n",
       "      <td>0.662222</td>\n",
       "      <td>0.558989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.573034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.688421</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.570225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.558989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.725108</td>\n",
       "      <td>0.626556</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.696774</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.687747</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C penalty     solver     Train      Test       OOT\n",
       "0  1      l2  newton-cg  0.688421  0.688596  0.550562\n",
       "1  1      l2  newton-cg  0.702407  0.662602  0.522472\n",
       "2  1      l2  newton-cg  0.682609  0.699588  0.553371\n",
       "3  1      l2  newton-cg  0.713389  0.662222  0.558989\n",
       "4  1      l2  newton-cg  0.698925  0.672269  0.573034\n",
       "5  1      l2  newton-cg  0.688421  0.692982  0.570225\n",
       "6  1      l2  newton-cg  0.689956  0.673469  0.558989\n",
       "7  1      l2  newton-cg  0.725108  0.626556  0.544944\n",
       "8  1      l2  newton-cg  0.696774  0.676471  0.550562\n",
       "9  1      l2  newton-cg  0.713333  0.687747  0.547753"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"1\"\n",
    "df_logistic['penalty']=\"l2\"\n",
    "df_logistic['solver']=\"newton-cg\"\n",
    "\n",
    "model = LogisticRegression(C=1,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"lbfgs\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter = 5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "prostate-drilling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.699934\n",
       "Test     0.674250\n",
       "OOT      0.553090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "finnish-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.691111</td>\n",
       "      <td>0.683794</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.715536</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.717195</td>\n",
       "      <td>0.639847</td>\n",
       "      <td>0.564607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.679842</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.708678</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.556180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.713348</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.709890</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>0.494382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.733871</td>\n",
       "      <td>0.570225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.684902</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.719027</td>\n",
       "      <td>0.653386</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C penalty     solver     Train      Test       OOT\n",
       "0  0.1      l2  newton-cg  0.691111  0.683794  0.547753\n",
       "1  0.1      l2  newton-cg  0.715536  0.646341  0.547753\n",
       "2  0.1      l2  newton-cg  0.717195  0.639847  0.564607\n",
       "3  0.1      l2  newton-cg  0.700000  0.679842  0.539326\n",
       "4  0.1      l2  newton-cg  0.708678  0.657534  0.556180\n",
       "5  0.1      l2  newton-cg  0.713348  0.682927  0.547753\n",
       "6  0.1      l2  newton-cg  0.709890  0.657258  0.494382\n",
       "7  0.1      l2  newton-cg  0.661538  0.733871  0.570225\n",
       "8  0.1      l2  newton-cg  0.684902  0.695122  0.542135\n",
       "9  0.1      l2  newton-cg  0.719027  0.653386  0.561798"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression(C=10,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"lbfgs\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "sweet-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.702122\n",
       "Test     0.672992\n",
       "OOT      0.547191\n",
       "dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "developing-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.696903</td>\n",
       "      <td>0.709163</td>\n",
       "      <td>0.519663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.677130</td>\n",
       "      <td>0.700389</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.698545</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.553371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.690171</td>\n",
       "      <td>0.689362</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.693790</td>\n",
       "      <td>0.699153</td>\n",
       "      <td>0.533708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.668161</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.693416</td>\n",
       "      <td>0.718894</td>\n",
       "      <td>0.556180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.695842</td>\n",
       "      <td>0.662602</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.680435</td>\n",
       "      <td>0.707819</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C penalty     solver     Train      Test       OOT\n",
       "0  10      l2  newton-cg  0.696903  0.709163  0.519663\n",
       "1  10      l2  newton-cg  0.683652  0.715517  0.550562\n",
       "2  10      l2  newton-cg  0.677130  0.700389  0.561798\n",
       "3  10      l2  newton-cg  0.698545  0.675676  0.553371\n",
       "4  10      l2  newton-cg  0.690171  0.689362  0.528090\n",
       "5  10      l2  newton-cg  0.693790  0.699153  0.533708\n",
       "6  10      l2  newton-cg  0.693750  0.668161  0.539326\n",
       "7  10      l2  newton-cg  0.693416  0.718894  0.556180\n",
       "8  10      l2  newton-cg  0.695842  0.662602  0.547753\n",
       "9  10      l2  newton-cg  0.680435  0.707819  0.539326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"10\"\n",
    "df_logistic['penalty']=\"l2\"\n",
    "df_logistic['solver']=\"newton-cg\"\n",
    "\n",
    "model = LogisticRegression(C=10,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"newton-cg\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "discrete-adaptation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.690363\n",
       "Test     0.694674\n",
       "OOT      0.542978\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "muslim-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.689218</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.536517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.728745</td>\n",
       "      <td>0.553371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.635193</td>\n",
       "      <td>0.553371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.700431</td>\n",
       "      <td>0.665272</td>\n",
       "      <td>0.533708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.539326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.711790</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.707889</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.536517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.681720</td>\n",
       "      <td>0.697479</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C penalty     solver     Train      Test       OOT\n",
       "0  100      l2  newton-cg  0.689218  0.686957  0.536517\n",
       "1  100      l2  newton-cg  0.675439  0.728745  0.553371\n",
       "2  100      l2  newton-cg  0.700210  0.690265  0.528090\n",
       "3  100      l2  newton-cg  0.723404  0.635193  0.553371\n",
       "4  100      l2  newton-cg  0.700431  0.665272  0.533708\n",
       "5  100      l2  newton-cg  0.676533  0.700000  0.539326\n",
       "6  100      l2  newton-cg  0.711790  0.653061  0.550562\n",
       "7  100      l2  newton-cg  0.707889  0.692308  0.536517\n",
       "8  100      l2  newton-cg  0.681720  0.697479  0.550562\n",
       "9  100      l2  newton-cg  0.702407  0.658537  0.528090"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"100\"\n",
    "df_logistic['penalty']=\"l2\"\n",
    "df_logistic['solver']=\"newton-cg\"\n",
    "\n",
    "model = LogisticRegression(C=100,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"newton-cg\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "twelve-techno",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.696904\n",
       "Test     0.680782\n",
       "OOT      0.541011\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "passing-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.728448</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.514045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.691974</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.704348</td>\n",
       "      <td>0.514045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.677895</td>\n",
       "      <td>0.688596</td>\n",
       "      <td>0.530899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.697095</td>\n",
       "      <td>0.505618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.691992</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.522472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.691983</td>\n",
       "      <td>0.659389</td>\n",
       "      <td>0.516854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.663507</td>\n",
       "      <td>0.536517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.522472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C penalty     solver     Train      Test       OOT\n",
       "0  0.01      l2  newton-cg  0.662420  0.728448  0.542135\n",
       "1  0.01      l2  newton-cg  0.666667  0.684000  0.514045\n",
       "2  0.01      l2  newton-cg  0.691974  0.665289  0.544944\n",
       "3  0.01      l2  newton-cg  0.676533  0.704348  0.514045\n",
       "4  0.01      l2  newton-cg  0.677895  0.688596  0.530899\n",
       "5  0.01      l2  newton-cg  0.666667  0.697095  0.505618\n",
       "6  0.01      l2  newton-cg  0.691992  0.652778  0.522472\n",
       "7  0.01      l2  newton-cg  0.691983  0.659389  0.516854\n",
       "8  0.01      l2  newton-cg  0.682927  0.663507  0.536517\n",
       "9  0.01      l2  newton-cg  0.683871  0.689076  0.522472"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic['C']=\"0.01\"\n",
    "df_logistic['penalty']=\"l2\"\n",
    "df_logistic['solver']=\"newton-cg\"\n",
    "\n",
    "model = LogisticRegression(C=0.01,\n",
    "                           penalty = 'l2',\n",
    "                           solver=\"newton-cg\",\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=5000)\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    df_logistic.loc[i:i,'Train':'OOT']=FDR(model,i)\n",
    "\n",
    "\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "verbal-brighton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.679293\n",
       "Test     0.683253\n",
       "OOT      0.525000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logistic[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-anime",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "municipal-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf=pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fitting-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.892405</td>\n",
       "      <td>0.864629</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.931111</td>\n",
       "      <td>0.857708</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.855967</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891258</td>\n",
       "      <td>0.850427</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.660112</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.833992</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.817427</td>\n",
       "      <td>0.609551</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.903158</td>\n",
       "      <td>0.785088</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.781513</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "8  0.892405  0.864629  0.646067            30         10            30\n",
       "4  0.931111  0.857708  0.648876            30         10            30\n",
       "6  0.871739  0.855967  0.646067            30         10            30\n",
       "7  0.891258  0.850427  0.643258            30         10            30\n",
       "1  0.905858  0.848889  0.660112            30         10            30\n",
       "0  0.906667  0.833992  0.643258            30         10            30\n",
       "3  0.928105  0.827869  0.637640            30         10            30\n",
       "9  0.904762  0.817427  0.609551            30         10            30\n",
       "5  0.903158  0.785088  0.629213            30         10            30\n",
       "2  0.909677  0.781513  0.637640            30         10            30"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=30\n",
    "max_depth=10\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='gini')\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "russian-surfing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.904474\n",
       "Test     0.832351\n",
       "OOT      0.640169\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "whole-creek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880383</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.880165</td>\n",
       "      <td>0.668539</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997904</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867220</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997912</td>\n",
       "      <td>0.861607</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853755</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995745</td>\n",
       "      <td>0.849785</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827309</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "3  1.000000  0.880383  0.643258            60         15            30\n",
       "8  0.997831  0.880165  0.668539            60         15            30\n",
       "7  0.997904  0.867257  0.643258            60         15            30\n",
       "1  1.000000  0.867220  0.640449            60         15            30\n",
       "6  1.000000  0.862661  0.643258            60         15            30\n",
       "4  0.997912  0.861607  0.640449            60         15            30\n",
       "5  1.000000  0.853755  0.651685            60         15            30\n",
       "9  0.995745  0.849785  0.634831            60         15            30\n",
       "2  1.000000  0.839357  0.654494            60         15            30\n",
       "0  1.000000  0.827309  0.643258            60         15            30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=60\n",
    "max_depth=15\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth,criterion='gini')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "accompanied-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.998939\n",
       "Test     0.858950\n",
       "OOT      0.646348\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "reverse-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899598</td>\n",
       "      <td>0.615169</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>0.601124</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866397</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.847737</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846774</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "7    1.0  0.899598  0.615169            60         30            30\n",
       "6    1.0  0.879845  0.626404            60         30            30\n",
       "0    1.0  0.877049  0.601124            60         30            30\n",
       "8    1.0  0.876543  0.654494            60         30            30\n",
       "4    1.0  0.868421  0.632022            60         30            30\n",
       "3    1.0  0.866397  0.617978            60         30            30\n",
       "9    1.0  0.864000  0.620787            60         30            30\n",
       "5    1.0  0.859031  0.626404            60         30            30\n",
       "2    1.0  0.847737  0.612360            60         30            30\n",
       "1    1.0  0.846774  0.623596            60         30            30"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=60\n",
    "max_depth=30\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='entropy')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "regulated-barbados",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.868540\n",
       "OOT      0.623034\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "exclusive-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914027</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.883817</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866379</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "7    1.0  0.914027  0.620787            80         15            30\n",
       "6    1.0  0.905172  0.612360            80         15            30\n",
       "2    1.0  0.895833  0.626404            80         15            30\n",
       "0    1.0  0.889868  0.620787            80         15            30\n",
       "9    1.0  0.887931  0.643258            80         15            30\n",
       "8    1.0  0.883817  0.629213            80         15            30\n",
       "5    1.0  0.866379  0.620787            80         15            30\n",
       "3    1.0  0.860759  0.612360            80         15            30\n",
       "4    1.0  0.855263  0.637640            80         15            30\n",
       "1    1.0  0.853556  0.626404            80         15            30"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=80\n",
    "max_depth=15\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='entropy')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "superb-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.881261\n",
       "OOT      0.625000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "failing-given",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899598</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.897541</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896861</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895735</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878151</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>0.660112</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857759</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "6    1.0  0.899598  0.620787            80         30            30\n",
       "3    1.0  0.897541  0.617978            80         30            30\n",
       "5    1.0  0.896861  0.643258            80         30            30\n",
       "0    1.0  0.895735  0.629213            80         30            30\n",
       "4    1.0  0.890756  0.626404            80         30            30\n",
       "2    1.0  0.890244  0.629213            80         30            30\n",
       "8    1.0  0.878151  0.620787            80         30            30\n",
       "9    1.0  0.878049  0.643258            80         30            30\n",
       "7    1.0  0.873913  0.660112            80         30            30\n",
       "1    1.0  0.857759  0.643258            80         30            30"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=80\n",
    "max_depth=30\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion = 'gini')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "portable-brother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.885861\n",
       "OOT      0.633427\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "modified-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872951</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862385</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "1  1.000000  0.893162  0.651685           100         15            30\n",
       "5  1.000000  0.875519  0.648876           100         15            30\n",
       "9  1.000000  0.872951  0.648876           100         15            30\n",
       "2  1.000000  0.871560  0.632022           100         15            30\n",
       "3  0.997938  0.871560  0.662921           100         15            30\n",
       "8  1.000000  0.870833  0.648876           100         15            30\n",
       "4  1.000000  0.868000  0.634831           100         15            30\n",
       "6  1.000000  0.864407  0.648876           100         15            30\n",
       "0  1.000000  0.862385  0.651685           100         15            30\n",
       "7  1.000000  0.836735  0.646067           100         15            30"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=100\n",
    "max_depth=15\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='gini')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "    \n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "willing-providence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.999794\n",
       "Test     0.868711\n",
       "OOT      0.647472\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "another-extreme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899123</td>\n",
       "      <td>0.603933</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893536</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.615169</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891775</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>0.609551</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886640</td>\n",
       "      <td>0.620787</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840816</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "6    1.0  0.899123  0.603933           100         30            30\n",
       "0    1.0  0.893536  0.620787           100         30            30\n",
       "3    1.0  0.892019  0.615169           100         30            30\n",
       "4    1.0  0.891775  0.632022           100         30            30\n",
       "7    1.0  0.889868  0.609551           100         30            30\n",
       "1    1.0  0.888889  0.643258           100         30            30\n",
       "5    1.0  0.886640  0.620787           100         30            30\n",
       "8    1.0  0.878505  0.623596           100         30            30\n",
       "2    1.0  0.870833  0.637640           100         30            30\n",
       "9    1.0  0.840816  0.640449           100         30            30"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=100\n",
    "max_depth=30\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth , criterion='entropy')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "\n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fifteen-harassment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.883200\n",
       "OOT      0.624719\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "significant-collins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.889344</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.660112</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866397</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860169</td>\n",
       "      <td>0.657303</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854772</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852814</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "5  1.000000  0.914530  0.646067           120         15            30\n",
       "8  1.000000  0.908696  0.651685           120         15            30\n",
       "4  1.000000  0.890756  0.654494           120         15            30\n",
       "7  0.997821  0.889344  0.646067           120         15            30\n",
       "2  1.000000  0.868421  0.660112           120         15            30\n",
       "9  1.000000  0.866397  0.651685           120         15            30\n",
       "0  1.000000  0.863118  0.640449           120         15            30\n",
       "6  1.000000  0.860169  0.657303           120         15            30\n",
       "3  1.000000  0.854772  0.651685           120         15            30\n",
       "1  1.000000  0.852814  0.629213           120         15            30"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=120\n",
    "max_depth=15\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth , criterion='gini')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "\n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "caroline-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.999782\n",
       "Test     0.876902\n",
       "OOT      0.648876\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "indoor-nightmare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.601124</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892562</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881857</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878543</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.598315</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871486</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "0    1.0  0.906122  0.612360           120         30            30\n",
       "3    1.0  0.904959  0.601124           120         30            30\n",
       "1    1.0  0.893162  0.637640           120         30            30\n",
       "6    1.0  0.892562  0.632022           120         30            30\n",
       "9    1.0  0.881857  0.626404           120         30            30\n",
       "4    1.0  0.878543  0.623596           120         30            30\n",
       "7    1.0  0.871560  0.598315           120         30            30\n",
       "5    1.0  0.871486  0.634831           120         30            30\n",
       "2    1.0  0.870690  0.612360           120         30            30\n",
       "8    1.0  0.846154  0.634831           120         30            30"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=120\n",
    "max_depth= 30\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='entropy')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "\n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "amended-homework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.881709\n",
       "OOT      0.621348\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "improving-assumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909804</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896694</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893004</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878661</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>0.634831</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.874419</td>\n",
       "      <td>0.615169</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.626404</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train      Test       OOT  n_estimators  max_depth  max_features\n",
       "9    1.0  0.933333  0.626404           150         30            30\n",
       "1    1.0  0.909804  0.640449           150         30            30\n",
       "2    1.0  0.898785  0.617978           150         30            30\n",
       "0    1.0  0.896694  0.634831           150         30            30\n",
       "8    1.0  0.893878  0.634831           150         30            30\n",
       "7    1.0  0.893004  0.617978           150         30            30\n",
       "6    1.0  0.878661  0.640449           150         30            30\n",
       "3    1.0  0.877049  0.634831           150         30            30\n",
       "5    1.0  0.874419  0.615169           150         30            30\n",
       "4    1.0  0.850394  0.626404           150         30            30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators=150\n",
    "max_depth= 30\n",
    "max_features=30\n",
    "model_rf=RandomForestClassifier(oob_score=True,n_estimators=n_estimators,max_depth=max_depth, criterion='entropy')\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    df_rf.loc[i:i,'Train':'OOT']=FDR(model_rf,i)\n",
    "\n",
    "df_rf['n_estimators']=n_estimators\n",
    "df_rf['max_depth']=max_depth\n",
    "df_rf['max_features']=max_features\n",
    "df_rf.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "taken-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    1.000000\n",
       "Test     0.890602\n",
       "OOT      0.628933\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-conjunction",
   "metadata": {},
   "source": [
    "# boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "interracial-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb=pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rational-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:14:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:14:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873874</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.578652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900662</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.889371</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.587079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897380</td>\n",
       "      <td>0.840816</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.909502</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901468</td>\n",
       "      <td>0.907080</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.899782</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>0.598315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.906725</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.900881</td>\n",
       "      <td>0.851406</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.873874  0.845560  0.578652\n",
       "1  0.900662  0.872000  0.601124\n",
       "2  0.889371  0.863636  0.587079\n",
       "3  0.897380  0.840816  0.603933\n",
       "4  0.909502  0.839080  0.612360\n",
       "5  0.901468  0.907080  0.603933\n",
       "6  0.899782  0.827869  0.598315\n",
       "7  0.906725  0.842975  0.612360\n",
       "8  0.900881  0.851406  0.623596\n",
       "9  0.901786  0.882353  0.615169"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "max_depth=3\n",
    "min_child_weight = 1\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adopted-might",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.898143\n",
       "Test     0.857278\n",
       "OOT      0.603652\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "played-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:15:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:15:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:16:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.751055</td>\n",
       "      <td>0.777293</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.774590</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.743158</td>\n",
       "      <td>0.767544</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.751634</td>\n",
       "      <td>0.725410</td>\n",
       "      <td>0.643258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.756322</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.778252</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.709163</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.780543</td>\n",
       "      <td>0.693487</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.686192</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "3  0.751055  0.777293  0.637640\n",
       "1  0.764706  0.774590  0.637640\n",
       "2  0.743158  0.767544  0.615169\n",
       "5  0.785714  0.741176  0.623596\n",
       "6  0.751634  0.725410  0.643258\n",
       "8  0.756322  0.723881  0.601124\n",
       "0  0.778252  0.717949  0.601124\n",
       "4  0.787611  0.709163  0.637640\n",
       "9  0.780543  0.693487  0.626404\n",
       "7  0.793103  0.686192  0.626404"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "max_depth=5\n",
    "min_child_weight = 3\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb\n",
    "df_xgb.sort_values('Test',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "honest-morocco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.769210\n",
       "Test     0.731668\n",
       "OOT      0.625000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "marked-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:17:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:17:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:18:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899787</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.882591</td>\n",
       "      <td>0.651685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.898230</td>\n",
       "      <td>0.856574</td>\n",
       "      <td>0.595506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.907173</td>\n",
       "      <td>0.838428</td>\n",
       "      <td>0.620787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.896328</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.910870</td>\n",
       "      <td>0.831276</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.901468</td>\n",
       "      <td>0.871681</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.896476</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.598315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.899787  0.871795  0.612360\n",
       "1  0.895397  0.866667  0.632022\n",
       "2  0.914474  0.882591  0.651685\n",
       "3  0.898230  0.856574  0.595506\n",
       "4  0.894040  0.852000  0.615169\n",
       "5  0.907173  0.838428  0.620787\n",
       "6  0.896328  0.845833  0.623596\n",
       "7  0.910870  0.831276  0.612360\n",
       "8  0.901468  0.871681  0.615169\n",
       "9  0.896476  0.819277  0.598315"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.05\n",
    "max_depth=6\n",
    "min_child_weight = 5\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "tribal-perspective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.901424\n",
       "Test     0.853612\n",
       "OOT      0.617697\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "accessory-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:18:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:19:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:19:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:19:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.856557</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.877395</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909483</td>\n",
       "      <td>0.887029</td>\n",
       "      <td>0.609551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.919831</td>\n",
       "      <td>0.864629</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915401</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.873518</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.909664</td>\n",
       "      <td>0.911894</td>\n",
       "      <td>0.581461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.911063</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.629213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.880851</td>\n",
       "      <td>0.598315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.907489</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.925926  0.856557  0.603933\n",
       "1  0.911765  0.877395  0.626404\n",
       "2  0.909483  0.887029  0.609551\n",
       "3  0.919831  0.864629  0.617978\n",
       "4  0.915401  0.871901  0.637640\n",
       "5  0.922222  0.873518  0.617978\n",
       "6  0.909664  0.911894  0.581461\n",
       "7  0.911063  0.871901  0.629213\n",
       "8  0.923077  0.880851  0.598315\n",
       "9  0.911765  0.907489  0.601124"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "max_depth=5\n",
    "min_child_weight = 7\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "handed-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.916020\n",
       "Test     0.880316\n",
       "OOT      0.612360\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "rational-illustration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:22:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764835</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.640449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.797710</td>\n",
       "      <td>0.646067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788793</td>\n",
       "      <td>0.744770</td>\n",
       "      <td>0.640449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783664</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.629213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.769874</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.620787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>0.629213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.775967</td>\n",
       "      <td>0.778302</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.780269</td>\n",
       "      <td>0.758755</td>\n",
       "      <td>0.609551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.776824</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.629213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.764835  0.750000  0.626404\n",
       "1  0.776119  0.752137  0.640449\n",
       "2  0.755102  0.797710  0.646067\n",
       "3  0.788793  0.744770  0.640449\n",
       "4  0.783664  0.740000  0.629213\n",
       "5  0.769874  0.791111  0.620787\n",
       "6  0.788462  0.774468  0.629213\n",
       "7  0.775967  0.778302  0.632022\n",
       "8  0.780269  0.758755  0.609551\n",
       "9  0.776824  0.759494  0.629213"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "max_depth=6\n",
    "min_child_weight = 1\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "south-concert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.775991\n",
       "Test     0.764675\n",
       "OOT      0.630337\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "canadian-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.820988</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.595506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.826271</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842650</td>\n",
       "      <td>0.786364</td>\n",
       "      <td>0.592697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815315</td>\n",
       "      <td>0.776062</td>\n",
       "      <td>0.589888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.856512</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.823276</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.808190</td>\n",
       "      <td>0.782427</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.834052</td>\n",
       "      <td>0.757322</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.809013</td>\n",
       "      <td>0.818565</td>\n",
       "      <td>0.606742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.820988  0.843318  0.595506\n",
       "1  0.826271  0.757576  0.626404\n",
       "2  0.842650  0.786364  0.592697\n",
       "3  0.815315  0.776062  0.589888\n",
       "4  0.856512  0.788000  0.601124\n",
       "5  0.814570  0.796000  0.603933\n",
       "6  0.823276  0.799163  0.617978\n",
       "7  0.808190  0.782427  0.603933\n",
       "8  0.834052  0.757322  0.617978\n",
       "9  0.809013  0.818565  0.606742"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.05\n",
    "max_depth=3\n",
    "min_child_weight = 3\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "approved-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.825084\n",
       "Test     0.790480\n",
       "OOT      0.605618\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "increased-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:24:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:24:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.937365</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.640449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.934685</td>\n",
       "      <td>0.888031</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.910931</td>\n",
       "      <td>0.629213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941432</td>\n",
       "      <td>0.834711</td>\n",
       "      <td>0.595506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.927505</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.935417</td>\n",
       "      <td>0.878924</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.940426</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.934461</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.936034</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.937365  0.883333  0.640449\n",
       "1  0.934685  0.888031  0.617978\n",
       "2  0.929825  0.910931  0.629213\n",
       "3  0.941432  0.834711  0.595506\n",
       "4  0.927505  0.888889  0.632022\n",
       "5  0.929032  0.924370  0.637640\n",
       "6  0.935417  0.878924  0.617978\n",
       "7  0.940426  0.862661  0.603933\n",
       "8  0.934461  0.900000  0.603933\n",
       "9  0.936034  0.893162  0.617978"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "max_depth=6\n",
    "min_child_weight = 5\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "pleasant-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.934618\n",
       "Test     0.886501\n",
       "OOT      0.619663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "palestinian-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:26:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:26:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.751606</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741304</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>0.558989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.713389</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.530899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.673387</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.710870</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.718884</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.530899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.747153</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.533708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.558989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.737069</td>\n",
       "      <td>0.698745</td>\n",
       "      <td>0.544944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.751606  0.694915  0.561798\n",
       "1  0.741304  0.724280  0.558989\n",
       "2  0.713389  0.697778  0.530899\n",
       "3  0.742857  0.673387  0.550562\n",
       "4  0.720779  0.717842  0.542135\n",
       "5  0.710870  0.641975  0.542135\n",
       "6  0.718884  0.696203  0.530899\n",
       "7  0.747153  0.715909  0.533708\n",
       "8  0.720000  0.671053  0.558989\n",
       "9  0.737069  0.698745  0.544944"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "max_depth=3\n",
    "min_child_weight = 7\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "rubber-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.730391\n",
       "Test     0.693209\n",
       "OOT      0.545506\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "soviet-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:27:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:28:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.889135</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896266</td>\n",
       "      <td>0.855204</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.877593</td>\n",
       "      <td>0.850679</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.894168</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.598315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.587079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.893013</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.829960</td>\n",
       "      <td>0.581461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.896247</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.606742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.888403</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.637640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.889135  0.833333  0.601124\n",
       "1  0.896266  0.855204  0.615169\n",
       "2  0.877593  0.850679  0.603933\n",
       "3  0.894168  0.837500  0.598315\n",
       "4  0.882759  0.861940  0.587079\n",
       "5  0.893013  0.877551  0.603933\n",
       "6  0.885965  0.829960  0.581461\n",
       "7  0.896247  0.832000  0.606742\n",
       "8  0.889126  0.833333  0.603933\n",
       "9  0.888403  0.833333  0.637640"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.05\n",
    "max_depth=5\n",
    "min_child_weight = 5\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "freelance-solution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.889267\n",
       "Test     0.844483\n",
       "OOT      0.603933\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "published-seven",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:29:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:30:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:30:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:31:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:31:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:31:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:31:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:31:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.896703</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.886918</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902386</td>\n",
       "      <td>0.838843</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.595506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.612360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.883227</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.893082</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.878319</td>\n",
       "      <td>0.840637</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.835391</td>\n",
       "      <td>0.589888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.893570</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.896703  0.814516  0.612360\n",
       "1  0.886918  0.857143  0.623596\n",
       "2  0.902386  0.838843  0.617978\n",
       "3  0.886364  0.835616  0.595506\n",
       "4  0.900000  0.814815  0.612360\n",
       "5  0.883227  0.875000  0.617978\n",
       "6  0.893082  0.849558  0.623596\n",
       "7  0.878319  0.840637  0.623596\n",
       "8  0.891304  0.835391  0.589888\n",
       "9  0.893570  0.817460  0.601124"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.05\n",
    "max_depth=6\n",
    "min_child_weight = 7\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "twenty-valentine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.891187\n",
       "Test     0.837898\n",
       "OOT      0.611798\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-pension",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "czech-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.729032</td>\n",
       "      <td>0.718487</td>\n",
       "      <td>0.514045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750533</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.581461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.525281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.542135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.776371</td>\n",
       "      <td>0.733624</td>\n",
       "      <td>0.595506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.741453</td>\n",
       "      <td>0.693617</td>\n",
       "      <td>0.547753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.745935</td>\n",
       "      <td>0.744076</td>\n",
       "      <td>0.558989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.742919</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.550562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.759414</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.516854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.491573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.729032  0.718487  0.514045\n",
       "1  0.750533  0.756410  0.581461\n",
       "2  0.717724  0.682927  0.525281\n",
       "3  0.783550  0.717842  0.542135\n",
       "4  0.776371  0.733624  0.595506\n",
       "5  0.741453  0.693617  0.547753\n",
       "6  0.745935  0.744076  0.558989\n",
       "7  0.742919  0.770492  0.550562\n",
       "8  0.759414  0.733333  0.516854\n",
       "9  0.727083  0.717489  0.491573"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'adam', alpha = 0.0001,\n",
    "                        hidden_layer_sizes = (5,), max_iter = 10000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)\n",
    "\n",
    "df_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "pointed-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.747402\n",
       "Test     0.726830\n",
       "OOT      0.542416\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "vanilla-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'sgd', alpha = 0.05,\n",
    "                        hidden_layer_sizes = (10, ), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "worthy-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.647841\n",
       "Test     0.658994\n",
       "OOT      0.502809\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "described-knight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', alpha = 0.0001,\n",
    "                        hidden_layer_sizes = (20, ), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "existing-edgar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.931422\n",
       "Test     0.790087\n",
       "OOT      0.451966\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "broken-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'adam', alpha = 0.05,\n",
    "                        hidden_layer_sizes = (5, 10), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "northern-things",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.749986\n",
       "Test     0.726475\n",
       "OOT      0.551966\n",
       "dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "favorite-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'sgd', alpha = 0.0001,\n",
    "                        hidden_layer_sizes = (5, 20), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "personal-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.654622\n",
       "Test     0.638009\n",
       "OOT      0.525281\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "attractive-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', alpha = 0.05,\n",
    "                        hidden_layer_sizes = (10, 10), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "absent-campaign",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.906895\n",
       "Test     0.794340\n",
       "OOT      0.481461\n",
       "dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "furnished-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'adam', alpha = 0.0001,\n",
    "                        hidden_layer_sizes = (10, 20), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "radical-relevance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.865793\n",
       "Test     0.797090\n",
       "OOT      0.546910\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "tender-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'sgd', alpha = 0.05,\n",
    "                        hidden_layer_sizes = (15, 10), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "defined-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.653660\n",
       "Test     0.656077\n",
       "OOT      0.517697\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "elect-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', alpha = 0.0001,\n",
    "                        hidden_layer_sizes = (15, 20), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "elementary-expression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.947605\n",
       "Test     0.777446\n",
       "OOT      0.433146\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "tamil-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(np.zeros(shape=(10,3)),columns=[\"Train\",\"Test\",\"OOT\"])\n",
    "\n",
    "\n",
    "\n",
    "model_nn = MLPClassifier(solver = 'adam', alpha = 0.05,\n",
    "                        hidden_layer_sizes = (20, 20), max_iter = 1000)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_nn.fit(x_train, y_train)\n",
    "    df_nn.loc[i:i,'Train':'OOT']=FDR(model_nn,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "illegal-bahrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train    0.838229\n",
       "Test     0.787962\n",
       "OOT      0.571067\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nn[['Train','Test','OOT']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-paper",
   "metadata": {},
   "source": [
    "# final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "distant-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:00:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:00:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:01:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:02:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:02:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:02:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.868313</td>\n",
       "      <td>0.609551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.940133</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.609551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.936543</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.601124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.937768</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.933045</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.620787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.937093</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.626404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.945295</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.946352</td>\n",
       "      <td>0.877637</td>\n",
       "      <td>0.643258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.933190</td>\n",
       "      <td>0.878661</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.868726</td>\n",
       "      <td>0.623596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train      Test       OOT\n",
       "0  0.939130  0.868313  0.609551\n",
       "1  0.940133  0.869048  0.609551\n",
       "2  0.936543  0.853659  0.601124\n",
       "3  0.937768  0.873418  0.623596\n",
       "4  0.933045  0.866667  0.620787\n",
       "5  0.937093  0.871901  0.626404\n",
       "6  0.945295  0.878049  0.632022\n",
       "7  0.946352  0.877637  0.643258\n",
       "8  0.933190  0.878661  0.632022\n",
       "9  0.941441  0.868726  0.623596"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "max_depth=6\n",
    "min_child_weight = 5\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(learning_rate= learning_rate, max_depth = max_depth, min_child_weight = min_child_weight)\n",
    "\n",
    "for i in range(10):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)\n",
    "    model_xgb.fit(x_train, y_train)\n",
    "    df_xgb.loc[i:i,'Train':'OOT']=FDR(model_xgb,i)\n",
    "    \n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ongoing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.34,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "declared-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:02:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "enabling-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model_xgb.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "difficult-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "neither-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Fraud'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fundamental-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred'] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "parental-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_desc_total_3</th>\n",
       "      <th>card_state_max_30</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>card_zip_max_1</th>\n",
       "      <th>merch_desc_total_0</th>\n",
       "      <th>card_desc_total_30</th>\n",
       "      <th>card_desc_total_14</th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "      <th>card_merch_max_1</th>\n",
       "      <th>...</th>\n",
       "      <th>merch_desc_total_1</th>\n",
       "      <th>merch_state_total_0</th>\n",
       "      <th>Merchnum_total_3</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_zip_max_0</th>\n",
       "      <th>card_zip_max_30</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>merch_desc_max_1</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31721</th>\n",
       "      <td>-0.113532</td>\n",
       "      <td>0.320341</td>\n",
       "      <td>-0.150262</td>\n",
       "      <td>-0.267437</td>\n",
       "      <td>-0.203542</td>\n",
       "      <td>-0.172416</td>\n",
       "      <td>-0.142702</td>\n",
       "      <td>-0.168118</td>\n",
       "      <td>-0.220156</td>\n",
       "      <td>-0.265179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201676</td>\n",
       "      <td>-0.216721</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.177467</td>\n",
       "      <td>-0.337534</td>\n",
       "      <td>-0.341005</td>\n",
       "      <td>-0.296187</td>\n",
       "      <td>-0.355252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43602</th>\n",
       "      <td>0.593168</td>\n",
       "      <td>1.264168</td>\n",
       "      <td>1.794770</td>\n",
       "      <td>1.891885</td>\n",
       "      <td>0.805608</td>\n",
       "      <td>0.491642</td>\n",
       "      <td>0.544799</td>\n",
       "      <td>0.666846</td>\n",
       "      <td>0.436490</td>\n",
       "      <td>1.897608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457326</td>\n",
       "      <td>0.790710</td>\n",
       "      <td>0.197049</td>\n",
       "      <td>0.488060</td>\n",
       "      <td>1.852642</td>\n",
       "      <td>1.668077</td>\n",
       "      <td>0.857217</td>\n",
       "      <td>1.576838</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66816</th>\n",
       "      <td>-0.096886</td>\n",
       "      <td>-0.429485</td>\n",
       "      <td>-0.157692</td>\n",
       "      <td>-0.280925</td>\n",
       "      <td>-0.208350</td>\n",
       "      <td>-0.136616</td>\n",
       "      <td>-0.105638</td>\n",
       "      <td>-0.172096</td>\n",
       "      <td>-0.154913</td>\n",
       "      <td>-0.278688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136199</td>\n",
       "      <td>-0.221521</td>\n",
       "      <td>-0.214033</td>\n",
       "      <td>-0.141588</td>\n",
       "      <td>-0.351215</td>\n",
       "      <td>-0.353555</td>\n",
       "      <td>-0.281322</td>\n",
       "      <td>-0.367321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29222</th>\n",
       "      <td>-0.027092</td>\n",
       "      <td>-0.149042</td>\n",
       "      <td>-0.091034</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>-0.080107</td>\n",
       "      <td>-0.091191</td>\n",
       "      <td>-0.058609</td>\n",
       "      <td>-0.065989</td>\n",
       "      <td>-0.109775</td>\n",
       "      <td>0.081674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090899</td>\n",
       "      <td>-0.093496</td>\n",
       "      <td>-0.204421</td>\n",
       "      <td>-0.096063</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>-0.018803</td>\n",
       "      <td>-0.243092</td>\n",
       "      <td>-0.045397</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31408</th>\n",
       "      <td>-0.151811</td>\n",
       "      <td>-0.530987</td>\n",
       "      <td>-0.195862</td>\n",
       "      <td>-0.424365</td>\n",
       "      <td>-0.201440</td>\n",
       "      <td>-0.208385</td>\n",
       "      <td>-0.179940</td>\n",
       "      <td>-0.213344</td>\n",
       "      <td>-0.135352</td>\n",
       "      <td>-0.422358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200303</td>\n",
       "      <td>-0.099635</td>\n",
       "      <td>-0.230622</td>\n",
       "      <td>-0.186854</td>\n",
       "      <td>-0.478363</td>\n",
       "      <td>-0.474713</td>\n",
       "      <td>-0.013223</td>\n",
       "      <td>-0.479485</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59773</th>\n",
       "      <td>-0.099759</td>\n",
       "      <td>1.209689</td>\n",
       "      <td>-0.160436</td>\n",
       "      <td>-0.212259</td>\n",
       "      <td>-0.183875</td>\n",
       "      <td>-0.159474</td>\n",
       "      <td>-0.129303</td>\n",
       "      <td>-0.151845</td>\n",
       "      <td>-0.207358</td>\n",
       "      <td>-0.209912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188832</td>\n",
       "      <td>-0.197087</td>\n",
       "      <td>-0.296653</td>\n",
       "      <td>-0.164496</td>\n",
       "      <td>-0.281567</td>\n",
       "      <td>0.455137</td>\n",
       "      <td>0.326427</td>\n",
       "      <td>-0.305880</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14034</th>\n",
       "      <td>-0.062909</td>\n",
       "      <td>-0.260888</td>\n",
       "      <td>-0.047470</td>\n",
       "      <td>-0.064629</td>\n",
       "      <td>-0.131253</td>\n",
       "      <td>-0.124847</td>\n",
       "      <td>-0.093453</td>\n",
       "      <td>-0.108306</td>\n",
       "      <td>-0.173117</td>\n",
       "      <td>-0.062046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154469</td>\n",
       "      <td>-0.144555</td>\n",
       "      <td>-0.270397</td>\n",
       "      <td>-0.129793</td>\n",
       "      <td>-0.131829</td>\n",
       "      <td>-0.152308</td>\n",
       "      <td>0.165906</td>\n",
       "      <td>-0.173786</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.190810</td>\n",
       "      <td>0.090885</td>\n",
       "      <td>-0.009590</td>\n",
       "      <td>-0.111634</td>\n",
       "      <td>1.139673</td>\n",
       "      <td>0.869172</td>\n",
       "      <td>-0.092074</td>\n",
       "      <td>-0.090819</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>-0.124970</td>\n",
       "      <td>-0.207291</td>\n",
       "      <td>1.137525</td>\n",
       "      <td>-0.076002</td>\n",
       "      <td>0.386862</td>\n",
       "      <td>-0.153187</td>\n",
       "      <td>-0.124538</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36358</th>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.059531</td>\n",
       "      <td>-0.027243</td>\n",
       "      <td>0.346440</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>-0.077776</td>\n",
       "      <td>0.349684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058785</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>-0.197290</td>\n",
       "      <td>-0.033162</td>\n",
       "      <td>0.285115</td>\n",
       "      <td>0.230160</td>\n",
       "      <td>0.425424</td>\n",
       "      <td>0.194025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48468</th>\n",
       "      <td>-0.036775</td>\n",
       "      <td>-0.308450</td>\n",
       "      <td>-0.100283</td>\n",
       "      <td>-0.125648</td>\n",
       "      <td>-0.113607</td>\n",
       "      <td>-0.100291</td>\n",
       "      <td>-0.068030</td>\n",
       "      <td>-0.120313</td>\n",
       "      <td>-0.131564</td>\n",
       "      <td>-0.123162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112766</td>\n",
       "      <td>-0.126939</td>\n",
       "      <td>-0.088440</td>\n",
       "      <td>-0.105182</td>\n",
       "      <td>-0.193719</td>\n",
       "      <td>-0.209081</td>\n",
       "      <td>-0.174211</td>\n",
       "      <td>-0.228383</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23476 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       card_desc_total_3  card_state_max_30  card_state_total_7  \\\n",
       "31721          -0.113532           0.320341           -0.150262   \n",
       "43602           0.593168           1.264168            1.794770   \n",
       "66816          -0.096886          -0.429485           -0.157692   \n",
       "29222          -0.027092          -0.149042           -0.091034   \n",
       "31408          -0.151811          -0.530987           -0.195862   \n",
       "...                  ...                ...                 ...   \n",
       "59773          -0.099759           1.209689           -0.160436   \n",
       "14034          -0.062909          -0.260888           -0.047470   \n",
       "3955            0.025663           0.190810            0.090885   \n",
       "36358           0.039700           0.059531           -0.027243   \n",
       "48468          -0.036775          -0.308450           -0.100283   \n",
       "\n",
       "       card_zip_max_1  merch_desc_total_0  card_desc_total_30  \\\n",
       "31721       -0.267437           -0.203542           -0.172416   \n",
       "43602        1.891885            0.805608            0.491642   \n",
       "66816       -0.280925           -0.208350           -0.136616   \n",
       "29222        0.078860           -0.080107           -0.091191   \n",
       "31408       -0.424365           -0.201440           -0.208385   \n",
       "...               ...                 ...                 ...   \n",
       "59773       -0.212259           -0.183875           -0.159474   \n",
       "14034       -0.064629           -0.131253           -0.124847   \n",
       "3955        -0.009590           -0.111634            1.139673   \n",
       "36358        0.346440            0.015270           -0.028429   \n",
       "48468       -0.125648           -0.113607           -0.100291   \n",
       "\n",
       "       card_desc_total_14  Cardnum_total_0  merch_state_total_1  \\\n",
       "31721           -0.142702        -0.168118            -0.220156   \n",
       "43602            0.544799         0.666846             0.436490   \n",
       "66816           -0.105638        -0.172096            -0.154913   \n",
       "29222           -0.058609        -0.065989            -0.109775   \n",
       "31408           -0.179940        -0.213344            -0.135352   \n",
       "...                   ...              ...                  ...   \n",
       "59773           -0.129303        -0.151845            -0.207358   \n",
       "14034           -0.093453        -0.108306            -0.173117   \n",
       "3955             0.869172        -0.092074            -0.090819   \n",
       "36358            0.006368         0.012926            -0.077776   \n",
       "48468           -0.068030        -0.120313            -0.131564   \n",
       "\n",
       "       card_merch_max_1  ...  merch_desc_total_1  merch_state_total_0  \\\n",
       "31721         -0.265179  ...           -0.201676            -0.216721   \n",
       "43602          1.897608  ...            0.457326             0.790710   \n",
       "66816         -0.278688  ...           -0.136199            -0.221521   \n",
       "29222          0.081674  ...           -0.090899            -0.093496   \n",
       "31408         -0.422358  ...           -0.200303            -0.099635   \n",
       "...                 ...  ...                 ...                  ...   \n",
       "59773         -0.209912  ...           -0.188832            -0.197087   \n",
       "14034         -0.062046  ...           -0.154469            -0.144555   \n",
       "3955          -0.006917  ...           -0.071875            -0.124970   \n",
       "36358          0.349684  ...           -0.058785             0.001719   \n",
       "48468         -0.123162  ...           -0.112766            -0.126939   \n",
       "\n",
       "       Merchnum_total_3  card_merch_total_30  merch_zip_max_0  \\\n",
       "31721         -0.306466            -0.177467        -0.337534   \n",
       "43602          0.197049             0.488060         1.852642   \n",
       "66816         -0.214033            -0.141588        -0.351215   \n",
       "29222         -0.204421            -0.096063         0.013711   \n",
       "31408         -0.230622            -0.186854        -0.478363   \n",
       "...                 ...                  ...              ...   \n",
       "59773         -0.296653            -0.164496        -0.281567   \n",
       "14034         -0.270397            -0.129793        -0.131829   \n",
       "3955          -0.207291             1.137525        -0.076002   \n",
       "36358         -0.197290            -0.033162         0.285115   \n",
       "48468         -0.088440            -0.105182        -0.193719   \n",
       "\n",
       "       card_zip_max_30  Cardnum_total_7  merch_desc_max_1  Fraud      pred  \n",
       "31721        -0.341005        -0.296187         -0.355252      0  0.000576  \n",
       "43602         1.668077         0.857217          1.576838      0  0.007196  \n",
       "66816        -0.353555        -0.281322         -0.367321      0  0.000671  \n",
       "29222        -0.018803        -0.243092         -0.045397      0  0.001765  \n",
       "31408        -0.474713        -0.013223         -0.479485      0  0.000304  \n",
       "...                ...              ...               ...    ...       ...  \n",
       "59773         0.455137         0.326427         -0.305880      0  0.000761  \n",
       "14034        -0.152308         0.165906         -0.173786      0  0.001323  \n",
       "3955          0.386862        -0.153187         -0.124538      1  0.559988  \n",
       "36358         0.230160         0.425424          0.194025      0  0.001938  \n",
       "48468        -0.209081        -0.174211         -0.228383      0  0.000350  \n",
       "\n",
       "[23476 rows x 32 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "frank-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "handled-collectible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_desc_total_3</th>\n",
       "      <th>card_state_max_30</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>card_zip_max_1</th>\n",
       "      <th>merch_desc_total_0</th>\n",
       "      <th>card_desc_total_30</th>\n",
       "      <th>card_desc_total_14</th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "      <th>card_merch_max_1</th>\n",
       "      <th>...</th>\n",
       "      <th>merch_desc_total_1</th>\n",
       "      <th>merch_state_total_0</th>\n",
       "      <th>Merchnum_total_3</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_zip_max_0</th>\n",
       "      <th>card_zip_max_30</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>merch_desc_max_1</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5048</th>\n",
       "      <td>-0.146328</td>\n",
       "      <td>-0.536633</td>\n",
       "      <td>-0.195452</td>\n",
       "      <td>-0.421549</td>\n",
       "      <td>-0.255298</td>\n",
       "      <td>-0.197160</td>\n",
       "      <td>-0.168319</td>\n",
       "      <td>-0.210491</td>\n",
       "      <td>-0.173095</td>\n",
       "      <td>-0.419537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229737</td>\n",
       "      <td>-0.256598</td>\n",
       "      <td>-0.270380</td>\n",
       "      <td>-0.190492</td>\n",
       "      <td>-0.494900</td>\n",
       "      <td>-0.481452</td>\n",
       "      <td>-0.322985</td>\n",
       "      <td>-0.493146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>-0.055403</td>\n",
       "      <td>-0.237450</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.034560</td>\n",
       "      <td>-0.120535</td>\n",
       "      <td>-0.117794</td>\n",
       "      <td>-0.086151</td>\n",
       "      <td>-0.099438</td>\n",
       "      <td>-0.166143</td>\n",
       "      <td>-0.031928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147470</td>\n",
       "      <td>-0.133855</td>\n",
       "      <td>-0.265050</td>\n",
       "      <td>-0.122724</td>\n",
       "      <td>-0.101330</td>\n",
       "      <td>-0.124331</td>\n",
       "      <td>-0.277099</td>\n",
       "      <td>-0.146881</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>-0.117010</td>\n",
       "      <td>-0.429832</td>\n",
       "      <td>-0.176912</td>\n",
       "      <td>-0.281370</td>\n",
       "      <td>-0.208509</td>\n",
       "      <td>-0.175684</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.122977</td>\n",
       "      <td>-0.223387</td>\n",
       "      <td>-0.279134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204919</td>\n",
       "      <td>-0.221679</td>\n",
       "      <td>-0.308944</td>\n",
       "      <td>-0.180743</td>\n",
       "      <td>-0.351666</td>\n",
       "      <td>-0.353969</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>-0.367718</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>-0.152978</td>\n",
       "      <td>-0.537450</td>\n",
       "      <td>-0.211264</td>\n",
       "      <td>-0.425462</td>\n",
       "      <td>-0.259869</td>\n",
       "      <td>-0.209481</td>\n",
       "      <td>-0.181075</td>\n",
       "      <td>-0.214723</td>\n",
       "      <td>-0.239171</td>\n",
       "      <td>-0.423457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220759</td>\n",
       "      <td>-0.272952</td>\n",
       "      <td>-0.321047</td>\n",
       "      <td>-0.214615</td>\n",
       "      <td>-0.497817</td>\n",
       "      <td>-0.143648</td>\n",
       "      <td>-0.331083</td>\n",
       "      <td>-0.442908</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60341</th>\n",
       "      <td>-0.142251</td>\n",
       "      <td>-0.533552</td>\n",
       "      <td>-0.186507</td>\n",
       "      <td>-0.422962</td>\n",
       "      <td>-0.251529</td>\n",
       "      <td>-0.195693</td>\n",
       "      <td>-0.166800</td>\n",
       "      <td>-0.199475</td>\n",
       "      <td>-0.044100</td>\n",
       "      <td>-0.420953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233012</td>\n",
       "      <td>0.053386</td>\n",
       "      <td>-0.036783</td>\n",
       "      <td>-0.167082</td>\n",
       "      <td>-0.100257</td>\n",
       "      <td>-0.477775</td>\n",
       "      <td>-0.317681</td>\n",
       "      <td>-0.494410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12774</th>\n",
       "      <td>-0.091521</td>\n",
       "      <td>0.442678</td>\n",
       "      <td>0.216577</td>\n",
       "      <td>-0.179255</td>\n",
       "      <td>0.112324</td>\n",
       "      <td>-0.115808</td>\n",
       "      <td>-0.084095</td>\n",
       "      <td>0.459277</td>\n",
       "      <td>-0.014624</td>\n",
       "      <td>-0.176855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.098608</td>\n",
       "      <td>-0.148865</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>-0.012759</td>\n",
       "      <td>-0.258958</td>\n",
       "      <td>0.367046</td>\n",
       "      <td>-0.068748</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.956088</td>\n",
       "      <td>0.400856</td>\n",
       "      <td>0.201795</td>\n",
       "      <td>0.327833</td>\n",
       "      <td>-0.059193</td>\n",
       "      <td>-0.025481</td>\n",
       "      <td>-0.025755</td>\n",
       "      <td>0.125606</td>\n",
       "      <td>0.204807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145327</td>\n",
       "      <td>0.313749</td>\n",
       "      <td>-0.006593</td>\n",
       "      <td>-0.063993</td>\n",
       "      <td>0.519786</td>\n",
       "      <td>0.095579</td>\n",
       "      <td>0.323504</td>\n",
       "      <td>0.401043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64762</th>\n",
       "      <td>-0.142478</td>\n",
       "      <td>0.424508</td>\n",
       "      <td>-0.112926</td>\n",
       "      <td>-0.383396</td>\n",
       "      <td>-0.244875</td>\n",
       "      <td>-0.197703</td>\n",
       "      <td>-0.168881</td>\n",
       "      <td>-0.202317</td>\n",
       "      <td>-0.247051</td>\n",
       "      <td>-0.381324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228667</td>\n",
       "      <td>-0.257984</td>\n",
       "      <td>-0.327089</td>\n",
       "      <td>-0.202810</td>\n",
       "      <td>-0.455150</td>\n",
       "      <td>-0.189847</td>\n",
       "      <td>-0.251270</td>\n",
       "      <td>-0.459008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21663</th>\n",
       "      <td>-0.044409</td>\n",
       "      <td>0.432527</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>-0.104837</td>\n",
       "      <td>-0.027502</td>\n",
       "      <td>-0.075457</td>\n",
       "      <td>-0.086450</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137218</td>\n",
       "      <td>-0.118183</td>\n",
       "      <td>-0.179013</td>\n",
       "      <td>-0.032233</td>\n",
       "      <td>-0.056658</td>\n",
       "      <td>0.675387</td>\n",
       "      <td>-0.230501</td>\n",
       "      <td>-0.107474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35318</th>\n",
       "      <td>-0.117173</td>\n",
       "      <td>-0.430340</td>\n",
       "      <td>-0.177068</td>\n",
       "      <td>-0.282022</td>\n",
       "      <td>-0.208741</td>\n",
       "      <td>-0.175837</td>\n",
       "      <td>-0.146243</td>\n",
       "      <td>-0.172420</td>\n",
       "      <td>-0.223538</td>\n",
       "      <td>-0.279787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205071</td>\n",
       "      <td>-0.221911</td>\n",
       "      <td>-0.309060</td>\n",
       "      <td>-0.180896</td>\n",
       "      <td>-0.352328</td>\n",
       "      <td>-0.354576</td>\n",
       "      <td>-0.306495</td>\n",
       "      <td>-0.368302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45570 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       card_desc_total_3  card_state_max_30  card_state_total_7  \\\n",
       "5048           -0.146328          -0.536633           -0.195452   \n",
       "31963          -0.055403          -0.237450           -0.118073   \n",
       "2528           -0.117010          -0.429832           -0.176912   \n",
       "4256           -0.152978          -0.537450           -0.211264   \n",
       "60341          -0.142251          -0.533552           -0.186507   \n",
       "...                  ...                ...                 ...   \n",
       "12774          -0.091521           0.442678            0.216577   \n",
       "29619           0.006962           0.956088            0.400856   \n",
       "64762          -0.142478           0.424508           -0.112926   \n",
       "21663          -0.044409           0.432527           -0.039493   \n",
       "35318          -0.117173          -0.430340           -0.177068   \n",
       "\n",
       "       card_zip_max_1  merch_desc_total_0  card_desc_total_30  \\\n",
       "5048        -0.421549           -0.255298           -0.197160   \n",
       "31963       -0.034560           -0.120535           -0.117794   \n",
       "2528        -0.281370           -0.208509           -0.175684   \n",
       "4256        -0.425462           -0.259869           -0.209481   \n",
       "60341       -0.422962           -0.251529           -0.195693   \n",
       "...               ...                 ...                 ...   \n",
       "12774       -0.179255            0.112324           -0.115808   \n",
       "29619        0.201795            0.327833           -0.059193   \n",
       "64762       -0.383396           -0.244875           -0.197703   \n",
       "21663        0.009482           -0.104837           -0.027502   \n",
       "35318       -0.282022           -0.208741           -0.175837   \n",
       "\n",
       "       card_desc_total_14  Cardnum_total_0  merch_state_total_1  \\\n",
       "5048            -0.168319        -0.210491            -0.173095   \n",
       "31963           -0.086151        -0.099438            -0.166143   \n",
       "2528            -0.146085        -0.122977            -0.223387   \n",
       "4256            -0.181075        -0.214723            -0.239171   \n",
       "60341           -0.166800        -0.199475            -0.044100   \n",
       "...                   ...              ...                  ...   \n",
       "12774           -0.084095         0.459277            -0.014624   \n",
       "29619           -0.025481        -0.025755             0.125606   \n",
       "64762           -0.168881        -0.202317            -0.247051   \n",
       "21663           -0.075457        -0.086450            -0.155929   \n",
       "35318           -0.146243        -0.172420            -0.223538   \n",
       "\n",
       "       card_merch_max_1  ...  merch_desc_total_1  merch_state_total_0  \\\n",
       "5048          -0.419537  ...           -0.229737            -0.256598   \n",
       "31963         -0.031928  ...           -0.147470            -0.133855   \n",
       "2528          -0.279134  ...           -0.204919            -0.221679   \n",
       "4256          -0.423457  ...           -0.220759            -0.272952   \n",
       "60341         -0.420953  ...           -0.233012             0.053386   \n",
       "...                 ...  ...                 ...                  ...   \n",
       "12774         -0.176855  ...            0.004594             0.098608   \n",
       "29619          0.204807  ...            0.145327             0.313749   \n",
       "64762         -0.381324  ...           -0.228667            -0.257984   \n",
       "21663          0.012185  ...           -0.137218            -0.118183   \n",
       "35318         -0.279787  ...           -0.205071            -0.221911   \n",
       "\n",
       "       Merchnum_total_3  card_merch_total_30  merch_zip_max_0  \\\n",
       "5048          -0.270380            -0.190492        -0.494900   \n",
       "31963         -0.265050            -0.122724        -0.101330   \n",
       "2528          -0.308944            -0.180743        -0.351666   \n",
       "4256          -0.321047            -0.214615        -0.497817   \n",
       "60341         -0.036783            -0.167082        -0.100257   \n",
       "...                 ...                  ...              ...   \n",
       "12774         -0.148865            -0.120734        -0.012759   \n",
       "29619         -0.006593            -0.063993         0.519786   \n",
       "64762         -0.327089            -0.202810        -0.455150   \n",
       "21663         -0.179013            -0.032233        -0.056658   \n",
       "35318         -0.309060            -0.180896        -0.352328   \n",
       "\n",
       "       card_zip_max_30  Cardnum_total_7  merch_desc_max_1  Fraud      pred  \n",
       "5048         -0.481452        -0.322985         -0.493146      0  0.000217  \n",
       "31963        -0.124331        -0.277099         -0.146881      0  0.001271  \n",
       "2528         -0.353969         0.003580         -0.367718      0  0.000325  \n",
       "4256         -0.143648        -0.331083         -0.442908      0  0.000161  \n",
       "60341        -0.477775        -0.317681         -0.494410      0  0.000255  \n",
       "...                ...              ...               ...    ...       ...  \n",
       "12774        -0.258958         0.367046         -0.068748      0  0.002373  \n",
       "29619         0.095579         0.323504          0.401043      0  0.002719  \n",
       "64762        -0.189847        -0.251270         -0.459008      0  0.000295  \n",
       "21663         0.675387        -0.230501         -0.107474      0  0.003091  \n",
       "35318        -0.354576        -0.306495         -0.368302      0  0.000489  \n",
       "\n",
       "[45570 rows x 32 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = model_xgb.predict_proba(x_train)[:,1]\n",
    "train_data = x_train.copy()\n",
    "train_data['Fraud'] = y_train\n",
    "train_data['pred'] = pred_train\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "driven-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "corrected-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oot = oot_data_selectded['Fraud']\n",
    "x_oot = oot_data_selectded.drop(columns = ['Fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "herbal-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_desc_total_3</th>\n",
       "      <th>card_state_max_30</th>\n",
       "      <th>card_state_total_7</th>\n",
       "      <th>card_zip_max_1</th>\n",
       "      <th>merch_desc_total_0</th>\n",
       "      <th>card_desc_total_30</th>\n",
       "      <th>card_desc_total_14</th>\n",
       "      <th>Cardnum_total_0</th>\n",
       "      <th>merch_state_total_1</th>\n",
       "      <th>card_merch_max_1</th>\n",
       "      <th>...</th>\n",
       "      <th>merch_desc_total_1</th>\n",
       "      <th>merch_state_total_0</th>\n",
       "      <th>Merchnum_total_3</th>\n",
       "      <th>card_merch_total_30</th>\n",
       "      <th>merch_zip_max_0</th>\n",
       "      <th>card_zip_max_30</th>\n",
       "      <th>Cardnum_total_7</th>\n",
       "      <th>merch_desc_max_1</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69461</th>\n",
       "      <td>0.043775</td>\n",
       "      <td>0.072255</td>\n",
       "      <td>-0.023352</td>\n",
       "      <td>0.362765</td>\n",
       "      <td>0.021089</td>\n",
       "      <td>-0.024600</td>\n",
       "      <td>0.010332</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>-0.073990</td>\n",
       "      <td>0.366034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054985</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>-0.194387</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0.301672</td>\n",
       "      <td>0.245349</td>\n",
       "      <td>0.109950</td>\n",
       "      <td>0.208632</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69462</th>\n",
       "      <td>-0.091425</td>\n",
       "      <td>-0.349935</td>\n",
       "      <td>-0.152476</td>\n",
       "      <td>-0.178869</td>\n",
       "      <td>-0.171973</td>\n",
       "      <td>-0.151642</td>\n",
       "      <td>-0.121194</td>\n",
       "      <td>-0.141998</td>\n",
       "      <td>0.295107</td>\n",
       "      <td>-0.176469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315436</td>\n",
       "      <td>-0.185206</td>\n",
       "      <td>1.133530</td>\n",
       "      <td>-0.156648</td>\n",
       "      <td>-0.247701</td>\n",
       "      <td>-0.258600</td>\n",
       "      <td>-0.078790</td>\n",
       "      <td>0.922761</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69463</th>\n",
       "      <td>-0.030843</td>\n",
       "      <td>-0.160757</td>\n",
       "      <td>-0.094617</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>-0.085465</td>\n",
       "      <td>-0.094716</td>\n",
       "      <td>-0.062259</td>\n",
       "      <td>-0.070421</td>\n",
       "      <td>-0.143323</td>\n",
       "      <td>0.066620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124568</td>\n",
       "      <td>-0.098844</td>\n",
       "      <td>-0.247551</td>\n",
       "      <td>-0.099596</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.032787</td>\n",
       "      <td>-0.263190</td>\n",
       "      <td>-0.058845</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69464</th>\n",
       "      <td>-0.153279</td>\n",
       "      <td>-0.540685</td>\n",
       "      <td>-0.208140</td>\n",
       "      <td>-0.426667</td>\n",
       "      <td>-0.260299</td>\n",
       "      <td>-0.208925</td>\n",
       "      <td>-0.180499</td>\n",
       "      <td>-0.215078</td>\n",
       "      <td>-0.257087</td>\n",
       "      <td>-0.424664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238739</td>\n",
       "      <td>-0.273381</td>\n",
       "      <td>-0.132321</td>\n",
       "      <td>-0.207381</td>\n",
       "      <td>-0.499039</td>\n",
       "      <td>-0.486288</td>\n",
       "      <td>-0.268645</td>\n",
       "      <td>-0.497726</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69465</th>\n",
       "      <td>-0.148955</td>\n",
       "      <td>-0.499662</td>\n",
       "      <td>-0.193287</td>\n",
       "      <td>-0.409345</td>\n",
       "      <td>-0.254125</td>\n",
       "      <td>-0.205701</td>\n",
       "      <td>-0.177162</td>\n",
       "      <td>-0.209970</td>\n",
       "      <td>-0.253069</td>\n",
       "      <td>-0.407314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234707</td>\n",
       "      <td>-0.267217</td>\n",
       "      <td>-0.331704</td>\n",
       "      <td>-0.210826</td>\n",
       "      <td>-0.481470</td>\n",
       "      <td>-0.473040</td>\n",
       "      <td>-0.321701</td>\n",
       "      <td>-0.482226</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96392</th>\n",
       "      <td>-0.133258</td>\n",
       "      <td>-0.480568</td>\n",
       "      <td>-0.192430</td>\n",
       "      <td>-0.346459</td>\n",
       "      <td>-0.231709</td>\n",
       "      <td>-0.190951</td>\n",
       "      <td>-0.161891</td>\n",
       "      <td>-0.191423</td>\n",
       "      <td>-0.238484</td>\n",
       "      <td>-0.344328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220069</td>\n",
       "      <td>-0.244840</td>\n",
       "      <td>-0.320520</td>\n",
       "      <td>-0.196043</td>\n",
       "      <td>-0.417685</td>\n",
       "      <td>-0.414529</td>\n",
       "      <td>-0.321193</td>\n",
       "      <td>-0.425958</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96393</th>\n",
       "      <td>-0.124881</td>\n",
       "      <td>0.520904</td>\n",
       "      <td>-0.003627</td>\n",
       "      <td>-0.312902</td>\n",
       "      <td>-0.219748</td>\n",
       "      <td>-0.044051</td>\n",
       "      <td>-0.009805</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>-0.230700</td>\n",
       "      <td>-0.310716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212258</td>\n",
       "      <td>-0.232899</td>\n",
       "      <td>-0.314552</td>\n",
       "      <td>-0.048818</td>\n",
       "      <td>-0.383648</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.105639</td>\n",
       "      <td>-0.395932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96394</th>\n",
       "      <td>-0.064497</td>\n",
       "      <td>-0.265848</td>\n",
       "      <td>-0.109833</td>\n",
       "      <td>-0.070993</td>\n",
       "      <td>1.774566</td>\n",
       "      <td>-0.126340</td>\n",
       "      <td>-0.094999</td>\n",
       "      <td>-0.110183</td>\n",
       "      <td>1.066983</td>\n",
       "      <td>-0.068419</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090082</td>\n",
       "      <td>1.758019</td>\n",
       "      <td>0.680510</td>\n",
       "      <td>-0.131289</td>\n",
       "      <td>1.990955</td>\n",
       "      <td>-0.158229</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>1.698852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96395</th>\n",
       "      <td>0.388974</td>\n",
       "      <td>1.150211</td>\n",
       "      <td>0.306335</td>\n",
       "      <td>1.745689</td>\n",
       "      <td>0.514023</td>\n",
       "      <td>0.425760</td>\n",
       "      <td>0.346152</td>\n",
       "      <td>0.425591</td>\n",
       "      <td>1.053309</td>\n",
       "      <td>1.751177</td>\n",
       "      <td>...</td>\n",
       "      <td>1.076360</td>\n",
       "      <td>0.499622</td>\n",
       "      <td>0.670026</td>\n",
       "      <td>0.422033</td>\n",
       "      <td>1.704356</td>\n",
       "      <td>1.532053</td>\n",
       "      <td>-0.025425</td>\n",
       "      <td>1.543743</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96396</th>\n",
       "      <td>0.798988</td>\n",
       "      <td>1.874612</td>\n",
       "      <td>0.697926</td>\n",
       "      <td>0.117822</td>\n",
       "      <td>-0.066219</td>\n",
       "      <td>0.685043</td>\n",
       "      <td>0.745027</td>\n",
       "      <td>-0.054498</td>\n",
       "      <td>-0.130801</td>\n",
       "      <td>0.120699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112000</td>\n",
       "      <td>-0.079632</td>\n",
       "      <td>-0.208572</td>\n",
       "      <td>-0.048075</td>\n",
       "      <td>0.053230</td>\n",
       "      <td>2.396735</td>\n",
       "      <td>0.569043</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26936 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       card_desc_total_3  card_state_max_30  card_state_total_7  \\\n",
       "69461           0.043775           0.072255           -0.023352   \n",
       "69462          -0.091425          -0.349935           -0.152476   \n",
       "69463          -0.030843          -0.160757           -0.094617   \n",
       "69464          -0.153279          -0.540685           -0.208140   \n",
       "69465          -0.148955          -0.499662           -0.193287   \n",
       "...                  ...                ...                 ...   \n",
       "96392          -0.133258          -0.480568           -0.192430   \n",
       "96393          -0.124881           0.520904           -0.003627   \n",
       "96394          -0.064497          -0.265848           -0.109833   \n",
       "96395           0.388974           1.150211            0.306335   \n",
       "96396           0.798988           1.874612            0.697926   \n",
       "\n",
       "       card_zip_max_1  merch_desc_total_0  card_desc_total_30  \\\n",
       "69461        0.362765            0.021089           -0.024600   \n",
       "69462       -0.178869           -0.171973           -0.151642   \n",
       "69463        0.063830           -0.085465           -0.094716   \n",
       "69464       -0.426667           -0.260299           -0.208925   \n",
       "69465       -0.409345           -0.254125           -0.205701   \n",
       "...               ...                 ...                 ...   \n",
       "96392       -0.346459           -0.231709           -0.190951   \n",
       "96393       -0.312902           -0.219748           -0.044051   \n",
       "96394       -0.070993            1.774566           -0.126340   \n",
       "96395        1.745689            0.514023            0.425760   \n",
       "96396        0.117822           -0.066219            0.685043   \n",
       "\n",
       "       card_desc_total_14  Cardnum_total_0  merch_state_total_1  \\\n",
       "69461            0.010332         0.017740            -0.073990   \n",
       "69462           -0.121194        -0.141998             0.295107   \n",
       "69463           -0.062259        -0.070421            -0.143323   \n",
       "69464           -0.180499        -0.215078            -0.257087   \n",
       "69465           -0.177162        -0.209970            -0.253069   \n",
       "...                   ...              ...                  ...   \n",
       "96392           -0.161891        -0.191423            -0.238484   \n",
       "96393           -0.009805         0.018099            -0.230700   \n",
       "96394           -0.094999        -0.110183             1.066983   \n",
       "96395            0.346152         0.425591             1.053309   \n",
       "96396            0.745027        -0.054498            -0.130801   \n",
       "\n",
       "       card_merch_max_1  ...  merch_desc_total_1  merch_state_total_0  \\\n",
       "69461          0.366034  ...           -0.054985             0.007528   \n",
       "69462         -0.176469  ...            0.315436            -0.185206   \n",
       "69463          0.066620  ...           -0.124568            -0.098844   \n",
       "69464         -0.424664  ...           -0.238739            -0.273381   \n",
       "69465         -0.407314  ...           -0.234707            -0.267217   \n",
       "...                 ...  ...                 ...                  ...   \n",
       "96392         -0.344328  ...           -0.220069            -0.244840   \n",
       "96393         -0.310716  ...           -0.212258            -0.232899   \n",
       "96394         -0.068419  ...            1.090082             1.758019   \n",
       "96395          1.751177  ...            1.076360             0.499622   \n",
       "96396          0.120699  ...           -0.112000            -0.079632   \n",
       "\n",
       "       Merchnum_total_3  card_merch_total_30  merch_zip_max_0  \\\n",
       "69461         -0.194387            -0.029324         0.301672   \n",
       "69462          1.133530            -0.156648        -0.247701   \n",
       "69463         -0.247551            -0.099596        -0.001534   \n",
       "69464         -0.132321            -0.207381        -0.499039   \n",
       "69465         -0.331704            -0.210826        -0.481470   \n",
       "...                 ...                  ...              ...   \n",
       "96392         -0.320520            -0.196043        -0.417685   \n",
       "96393         -0.314552            -0.048818        -0.383648   \n",
       "96394          0.680510            -0.131289         1.990955   \n",
       "96395          0.670026             0.422033         1.704356   \n",
       "96396         -0.208572            -0.048075         0.053230   \n",
       "\n",
       "       card_zip_max_30  Cardnum_total_7  merch_desc_max_1  Fraud      pred  \n",
       "69461         0.245349         0.109950          0.208632      0  0.001648  \n",
       "69462        -0.258600        -0.078790          0.922761      0  0.000296  \n",
       "69463        -0.032787        -0.263190         -0.058845      0  0.001509  \n",
       "69464        -0.486288        -0.268645         -0.497726      0  0.000266  \n",
       "69465        -0.473040        -0.321701         -0.482226      0  0.000172  \n",
       "...                ...              ...               ...    ...       ...  \n",
       "96392        -0.414529        -0.321193         -0.425958      0  0.000269  \n",
       "96393         0.059015         0.105639         -0.395932      0  0.000503  \n",
       "96394        -0.158229         0.027114          1.698852      0  0.035137  \n",
       "96395         1.532053        -0.025425          1.543743      0  0.003192  \n",
       "96396         2.396735         0.569043         -0.010535      0  0.088755  \n",
       "\n",
       "[26936 rows x 32 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_oot = model_xgb.predict_proba(x_oot)[:,1]\n",
    "oot_data = x_oot.copy()\n",
    "oot_data['Fraud'] = y_oot\n",
    "oot_data['pred'] = pred_oot\n",
    "oot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fluid-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot_data.to_csv('oot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
